\chapter{Tiling Adverbs\label{chap:tiling_adverbs}}

\section{Example: Tiling Adverbial Matrix Multiplication\label{sec:adverb_mm}}

In order to understand how our compiler automatically tiles Parakeet code, let us walk through an example.  In Figure \ref{parmm}, we see an implementation of matrix multipication.  This is a desugared version of the actual Parakeet code that we used in our benchmarks in Chapter \ref{chap:evaluation}, altered for ease of presentation to assume that the second input matrix \lstinline{Y} has already been transposed.

\begin{figure}
\begin{singlespace}
\begin{lstlisting}
def dot(x, y):
  return Reduce(lambda x,y,acc: acc + x*y,
                x, y, init=0,
                combiner=lambda x,y: x+y)

def dgemm_1(x, Y):
  return Map(dot, Y, fixed=[x])
                
def dgemm(X, Y):
  return Map(dgemm_1, X, fixed=[Y])
\end{lstlisting}
\end{singlespace}
\label{parmm}
\caption{Parakeet Matrix Multiplication}
\end{figure}

Recall from the discussion in Chapter \ref{chap:optimizations} that the \lstinline{AllPairs} pattern of data access in matrix multiplication involves temporal reuse such that cache tiling is very beneficial for performance.  At a high level, tiling an adverb (or loop) involves wrapping that adverb in another version of itself that breaks up its iteration pattern into smaller pieces.  The outer adverb iterates over tiles, while the inner one performs the original computation on the individual elements of each tile.

To support tiling in our compiler we introduce a set of \emph{tiled adverbs}, one for each existing adverb.  At a high level, the semantics of a tiled adverb are that it splits its inputs along some axis into tiles.  These tiles are then each consumed by executing the tiled adverb's nested function, and these partial results are combined in some structured way to form the final result.  Definitions of tiled adverbs' semantics are given in Figure \ref{def_tiled_adverbs}.  Note that our actual implementations of tiled adverbs are more efficient than these definitions might suggest.  The \lstinline{Split} operator in Figure \ref{def_tiled_adverbs} splits its arguments into tiles along the specified axis.  However, the actual tile sizes are left unspecified -- in the case of cache tiling until runtime, and in the case of register tiling until final compile time.

To see the problem with this, let's examine its cache behavior in detail.  Assume that we're tiling for L1 cache and that the matrices are much larger than the size of L1 cache so that even a single row from either matrix is on the order of the size of the cache (a realistic assumption for even medium sized data and current cache sizes).

First, the rows of the matrices \lstinline{X} and \lstinline{Y} are split into tiles.  Then, for every pair of tiles, every pair of rows from the tiles is passed to the function \lstinline{tiled_dot}.  At this point we can see that the \lstinline{TiledAllPairs} isn't going to do much to help cache behavior, as the single rows of the matrices are already large compared with the cache and so grouping them into even larger tiles is useless.

The next step of the computation involves splitting each pair of rows from \lstinline{X} and \lstinline{Y} into tiles, and then performing a dot product between each of these pieces.  Finally, these partial dot products are combined via addition to produce a final result.  In contrast with the \lstinline{TiledAllPairs}, the tiles from the \lstinline{TiledReduce} are able to benefit cache behavior, as they can be made small enough to fit into cache.  However, since they are splitting single rows of \lstinline{X} and \lstinline{Y}, the tiles' shapes are forced to be 1xN for some N.  It has been shown in various studies of tiling that the best tile shapes are typically closer to square, and ATLAS only considers square tiles in its search (REFS).  Regardless, we want the runtime's search to be able to find the best tile shapes and thus prefer not to restrict their shape in any way.

Thus we see that this straightforward approach to tiling the adverbs is problematic.  What we'd really prefer is for all of the tiling of every adverb to happen first, and then once the inputs are broken up into cache-friendly pieces only then to perform the inner computation.  A tiled version of matrix multiply that adheres to this principle is shown in Listing \ref{good_tiled_adverb_mm}.

\begin{figure}
\begin{singlespace}
\begin{lstlisting}
def dot(x, y):
  return Reduce(lambda x,y,acc: acc + x*y,
                x, y, init=0,
                combiner=lambda x,y: x+y)

def dgemm(X, Y):
  return AllPairs(dot, X, Y)

def tiled_dot(X, Y):
  return TiledReduce(dgemm, X, Y,
                     init=0, axes=[1,1],
                     combiner=lambda x,y: x+y)

def tiled_dgemm_1(x, Y):
  return TiledMap(tiled_dot, Y, fixed=[x])
                     
def tiled_dgemm(X, Y):
  return TiledMap(tiled_dgemm_1, X, fixed=[Y])
\end{lstlisting}
\end{singlespace}
\label{good_tiled_adverb_mm}
\caption{Good Tiled Matrix Multiplication}
\end{figure}

The difference between this code and the last is that it's organized in the way we want: all of the tiling happens first, and the inner computation is performed on the cache-friendly pieces of the inputs. This solves both problems with the first version: the tiling of the \lstinline{TiledAllPairs} is now relevant in fitting the inner computation into cache, and all axes of the cache-relevant tiles' shapes are now free parameters that are settable at runtime.

\section{Adverb Tiling Algorithm\label{sec:adverb_tiling_algorithm}}

While the transformation ``nest all tilings, then all original computation'' is legal in the case of matrix multiply, the case of general code is more complicated.  This is due to various factors such as the presence of additional non-adverb statements in functions which are tiled and control flow.  The full algorithm for our tiling transformation is given if Figure \ref{tiling_algorithm}.

\begin{figure*}
\centering
\begin{tabular}{| m{0.01cm}llm{10.8cm} |}
\hline
& & &\\
& function   & $::=$ & $\lambda x_1,\ldots,x_n.$statement$^+$\\
& statement  & $::=$ & $x$ \textbf{= } expression $|$ \textbf{return} expression\\
& expression & $::=$ & \textbf{map}($f$, $e_1,\ldots,e_n$) $|$ \textbf{reduce}($f$, $combiner, e_{init}, e_1,\ldots,e_n$)\\
&            &       & $|$ $e_1 \oplus e_2 $ $|$ $e_1[e_2]$ $|$ $x$\\
& & &\\
\hline
\end{tabular}
\caption{Simplified Parakeet Syntax}
\label{tiling_syntax}
\end{figure*}

\begin{figure*}
\centering
\begin{tabular}{| m{0.01cm}llm{9.5cm} |}
\hline
& & &\\
& $\llbracket\textbf{\textrm{return}}$ $e\rrbracket^{\alpha,\epsilon}_{stmt}$ & $::=$ & \textbf{return} $\llbracket e \rrbracket^{\alpha,\epsilon}_{expr}$\\
& & &\\
\hline
\end{tabular}
\caption{Adverb Tiling Transformation Algorithm}
\label{tiling_algorithm}
\end{figure*}

Before tiling an AST, we normalize every compound expression by breaking it up into a series of statements each of which only contains simple expressions.

\section{Comparison with the Polyhedral Model\label{sec:polyhedral_comparison}}

The Polyhedral Model is a well-studied method for modeling the iteration spaces and data dependencies of statements in loop nests.  The high level model represents these as a series of matrices which enables performing various loop optimizations including tiling, skewing, and automatic parallelization via algebraic transformations of these matrices~\cite{Bena10,Bond08,Hart09,Pouc10,Wolf91a,Wolf91b}.

