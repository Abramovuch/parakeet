\documentclass[preprint,9pt]{sigplanconf}

\usepackage{graphicx,listings,fixltx2e,lambda,array,times, color}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{color}

\definecolor{lightgray}{rgb}{0.92,0.92,0.92}

\lstset{ 
  language=Python,                % the language of the code
  linewidth=220pt, 
  xleftmargin=8pt,
  basicstyle=\footnotesize\ttfamily, % Standardschrift
  numbers=none,                   % where to put the line-numbers
  numberstyle=\footnotesize,          % the size of the fonts that are used for the line-numbers
  stepnumber=1,                   % the step between two line-numbers. If it's 1, each line 
                                  % will be numbered
  numbersep=5pt,                  % how far the line-numbers are from the code
  showspaces=false,               % show spaces adding particular underscores
  showstringspaces=false,         % underline spaces within strings
  showtabs=false,                 % show tabs within strings adding particular underscores
  frame=single,                   % adds a frame around the code
  tabsize=2,                      % sets default tabsize to 2 spaces
  captionpos=b,                   % sets the caption-position to bottom
  breaklines=true,                % sets automatic line breaking
  breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
  title=\lstname,                   % show the filename of files included with \lstinputlisting;
                                  % also try caption instead of title
  numberstyle=\tiny\color{gray},        % line number style
  keywordstyle=\color{blue},          % keyword style
  commentstyle=\color{dkgreen},       % comment style
  backgroundcolor=\color{lightgray}, 
  belowskip=-10pt, 
  aboveskip=6pt, 
}

\begin{document}

\conferenceinfo{PLDI '13}{16--21 June 2013, Seattle, WA.} 
\copyrightyear{2013} 
\copyrightdata{[to be supplied]} 

\titlebanner{}        % These are ignored unless
\preprintfooter{PLDI 2013}   % 'preprint' option specified.

\title{Automated Online Tuning of High Level Numerical Programs}

\authorinfo{Name Omitted For Review}
           {Affiliation Omitted For Review}
           {omitted@omitted.com}

% define some useful commands to use in language specification 
\newcommand{\MAP}{\impfnt{map}}
\newcommand{\REDUCE}{\impfnt{reduce}}
\newcommand{\SCAN}{\impfnt{scan}}
\newcommand{\ALLPAIRS}{\impfnt{allpairs}}
\newcommand{\concat}{\ensuremath{+\!\!\!\!+\,}}

\maketitle

\begin{abstract}
The current best practice for tuning numerical code performance involves offline auto-tuning to find optimal settings of various parameters including tile sizes and loop unrolling factors.  However, this approach burdens programmers in at least one of two ways: (1) it requires users either to annotate their code mannually with information about its tunable parameters, or to implement by hand the hooks which an auto-tuner can use to explore different parameter settings; or (2) it involves a time-consuming offline search that may not be worth the cost for code not reused extensively.  These approaches are thus poorly suited for both non-expert programmers (who cannot annotate their code in the necessary way) and rapid prototyping (a common use case of numerical programming).

We present a system for accelerating numerical Python programs that suffers from neither of these drawbacks. Programmers write code using Parakeet, a NumPy-like DSL for numerical programming embedded in Python that includes higher-order data parallel primitives (referred to as \emph{adverbs}) including $\MAP$, $\REDUCE$, and $\SCAN$.  For each nesting of adverbs, one adverb is parallelized by executing multiple iterations on different threads while all others are sequentialized into loops.  For sequentialized adverbs, we extend Parakeet's JIT compiler with a novel code transformation that facilitates automatically inserting auto-tuning hooks.  These hooks enable an efficient online search across different parameter settings for cache tiling, register blocking, and loop unrolling.  Our system thus introduces the benefits of auto-tuning to an environment suited to both non-expert programmers and rapid prototyping.

We evaluate our system on a suite of benchmark programs written in Parakeet+Python and show an average XX\% performance improvement over code that uses an analytic model to determine the best settings of the tuning parameters statically, and only an average YY\% performance degredation from expert-tuned C code.
\end{abstract}

%\category{CR-number}{subcategory}{third-level}

%\terms
%term1, term2

%\keywords
%Compilers, Auto-Tuning, Online Optimization, Memory hierarchy, Tiling, Blocking, Unrolling, Program transformation, Code generation

\section{Introduction}
We extend the work of \cite{Rubi12} with support for parallel execution on CPUs.

Previous work has shown that a given parallel application -- even one as simple as the embarrassingly parallel Black-Scholes option pricing -- can have wildly varying performance across different input data sets~\cite{Rama12}.

\section{The Parakeet API and Compiler}
We generate code using the LLVM compiler infrastructure~\cite{Latt02}.

\section{Optimizations}

\subsection{Tiling Optimizations}

\subsection{Loop Unrolling Optimizations}

\section{Parallelization and Online Tuning}

\section{Evaluation}
Evaluate System on MM, K-Means, some Map-based Programs, Natural Join, and SVM, presenting overall performance improvements vs hand-tuned C and vs performance without any search and vs cherry-picked best parameter settings.

Make sure to present numbers on the exact overheads of recompilation and job reconfiguration.

Present exploration vs exploitation graphs.

\section{Related Work}
Our work builds upon a range of existing fields of study including just-in-time compilation, acceleration of high level array languages, loop optimizations, analytic performance modeling, and auto-tuning.

JIT compilation.

Python acceleration and the like (such as Numpy~\cite{Dubo96}, Numba, MaJIC~\cite{Alma02}, and Copperhead~\cite{Cata11}).

There has been extensive research on loop optimizations, including cache tiling, register tiling, data copying, data padding, and loop unrolling optimizations (e.g.~\cite{Lam91, Wolf91}). (Probably need a few more refs).  Our system builds on this body of work by providing a framework for automating optimization parameter selection, performing the search for good parameter settings online, and extending the applicability of these optimizations to unannotated, high-level code.

Analytic models for determining parameter settings for dense matrix multiplication~\cite{Cole95, Yoto03, Yoto05}.  Wolf et al.~developed an analytic model for use in a compiler to determine tiling and loop unrolling settings statically for sequential C and Fortran programs~\cite{Wolf96}.  Recent work on analytical bounds for optimal tile sizes~\cite{Shir12}.

In recent years, offline auto-tuning has emerged as the accepted best practice for optimizing numerical code~\cite{Asan06}.  Libraries such as ATLAS for dense linear algebra~\cite{Atlas} and FFTW for Fourier transforms~\cite{Frig05} deliver the best performance available across a wide range of architectures and platforms for their specific problem domains via an extensive offline search performed at installation time.  Other recent work on offline auto-tuning includes that for stencil computations~\cite{Kami10,Datt08}.  While this technique is useful for either basic computational building blocks that are reused extensively (ATLAS and FFTW) or in the hands of expert programmers that are able to hand-program efficient low-level code with auto-tuning hooks inserted, our system expands the usefulness of auto-tuning to a broader audience and to rapid prototyping scenarios.

Chen et al. developed a system that automatically generates multiple candidate versions of C and Fortran programs by analyzing array references in loops and performing unroll-and-jam, cache tiling, copying, and TLB-oriented optimizations~\cite{Chen05b}.  Their focus is very much in the spirit of our work, but its offline search takes on the order of minutes and its algorithm for deriving parameterized code variants involves complex analysis of memory references and footprints.  We show that excellent performance is possible via a simple analytic model combined with efficient online search across parameter settings.

Active Harmony is a system for performing online auto-tuning that exposes a constraint specification language (CSL) for expressing tuning parameters~\cite{Tiwa11}.  The Active Harmony system doesn't address the problem of generating these tuning parameters, and its evaluation involved manual programming of the tuning parameters in the CSL as well as manual outlining of compute-intensive functions.  Active Harmony also targets loop-based programs written in C and Fortran.  Our system automatically inserts the necessary hooks for auto-tuning, and automatically extracts code sections from high level Python programs.  Thus we are able to improve performance of code written by non-expert programmers written in a high level language.  Finally, Active Harmony's recompilation is relatively heavy-weight and thus targets distributed clusters where it is possible to have dedicated code-generation servers.  In our system code generation is cheaper and is co-located with program execution on a single shared-memory node.

Tiwari et al.~presented a system that combines Active Harmony and Chen's system to auto-tune loop-based C and Fortran programs offline~\cite{Tiwa09}, thus targeting different use cases than does our system.

Knijnenburg et al.~presented a system for combining analytic models with offline auto-tuning to improve parameter settings of loop tiling and unrolling optimizations~\cite{Knij04}.  While they focus on two of the same optimizations as we do and use analytic models to improve their search, their search is performed offline and requires on the order of minutes to finish.

\section{Conclusion}
We have presented a system that automatically generates auto-tuning hooks in just-in-time compiled versions of code written in Parakeet+Python, a high-level numerical productivity language.  Our system automatically searches across different tuning parameter settings online, resulting in large performance improvements for a wide range of numerical programs.  We thus bring cutting-edge performance to non-expert programmers and to a rapid prototyping environment.

%\appendix
%\section{Appendix Title}

%This is the text of the appendix, if you need one.

%\acks

%Acknowledgments, if needed.

\bibliographystyle{abbrvnat}
\bibliography{../Parallelism}{}

% The bibliography should be embedded for final submission.

%\begin{thebibliography}{}
%\softraggedright

%\bibitem[Smith et~al.(2009)Smith, Jones]{smith02}
%P. Q. Smith, and X. Y. Jones. ...reference text...

%\end{thebibliography}

\end{document}
